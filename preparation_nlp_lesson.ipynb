{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a827504-a20b-4913-9598-e07d70b298cf",
   "metadata": {},
   "source": [
    "# DATA PREPARATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f22880-d2f7-4d0e-85b5-6ba5efec9eaf",
   "metadata": {},
   "source": [
    "### Here's our plan for parsing the text data:\n",
    "\n",
    "- Convert text to all lower case for normalcy.\n",
    "- Remove any accented characters, non-ASCII characters.\n",
    "- Remove special characters.\n",
    "- Stem or lemmatize the words.\n",
    "- Remove stopwords.\n",
    "- Store the clean text and the original text for use in future notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25e0bbaf-f403-42a4-a996-75960817922c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import acquire_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ce4cc28-3094-4b27-a12c-7b91a1ce3ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Coming into our Data Science program, you will need to know some math and stats. However, many of our applicants actually learn in the application process – you don’t need to be an expert before applying! Data science is a very accessible field to anyone dedicated to learning new skills, and we can work with any applicant to help them learn what they need to know. But what “skills” do we mean, exactly? Just what exactly are the data science math and stats principles you need to know?', 'What are the main math principles you need to know to get into Codeup’s Data Science program?'\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article = \"Coming into our Data Science program, you will need to know some math and \\\n",
    "stats. However, many of our applicants actually learn in the application process – you \\\n",
    "don’t need to be an expert before applying! Data science is a very accessible field to \\\n",
    "anyone dedicated to learning new skills, and we can work with any applicant to help them \\\n",
    "learn what they need to know. But what “skills” do we mean, exactly? Just what exactly \\\n",
    "are the data science math and stats principles you need to know?', 'What are the main \\\n",
    "math principles you need to know to get into Codeup’s Data Science program?'\"\n",
    "article"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6632b07-e9ef-4d08-baad-160e34a466ff",
   "metadata": {},
   "source": [
    "- Convert text to all lower case for normalcy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4b73e1c-8b3c-4e5e-89e5-b741788757e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coming into our data science program, you will need to know some math and stats. however, many of our applicants actually learn in the application process – you don’t need to be an expert before applying! data science is a very accessible field to anyone dedicated to learning new skills, and we can work with any applicant to help them learn what they need to know. but what “skills” do we mean, exactly? just what exactly are the data science math and stats principles you need to know?', 'what are the main math principles you need to know to get into codeup’s data science program?'\n"
     ]
    }
   ],
   "source": [
    "article = article.lower()\n",
    "print(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be697eb5-3f0b-4c59-8585-de18f5fa0a3b",
   "metadata": {},
   "source": [
    "### Removing accented characters\n",
    "\n",
    " - Usually in any text corpus, you might be dealing with accented characters/letters, especially if you only want to analyze the English language. Hence, we need to make sure that these characters are converted and standardized into ASCII characters. A simple example is converting é to e.\n",
    "\n",
    "####  We'll go about this in three steps:\n",
    "\n",
    "* `unicodedata.normalize` removes any inconsistencies in unicode character encoding.\n",
    "  \n",
    "    * `.encode` to convert the resulting string to the ASCII character set. We'll ignore any errors in conversion, meaning we'll drop anything that isn't an ASCII character.\n",
    "    * `.decode` to turn the resulting `bytes` object back into a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83acb11a-91b3-45f9-aadc-c87bbc7a7a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coming into our data science program, you will need to know some math and stats. however, many of our applicants actually learn in the application process  you dont need to be an expert before applying! data science is a very accessible field to anyone dedicated to learning new skills, and we can work with any applicant to help them learn what they need to know. but what skills do we mean, exactly? just what exactly are the data science math and stats principles you need to know?', 'what are the\n"
     ]
    }
   ],
   "source": [
    "article = unicodedata.normalize('NFKD', article)\\\n",
    "    .encode('ascii', 'ignore')\\\n",
    "    .decode('utf-8', 'ignore')\n",
    "\n",
    "print(article[0:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0562354-6118-4c9b-8b70-12a9e37b36bc",
   "metadata": {},
   "source": [
    "### Removing Special Characters\n",
    "- **Special characters and symbols are usually non-alphanumeric characters or even occasionally numeric characters (depending on the problem), which add to the extra noise in unstructured text. Usually, simple regular expressions (regexes) can be used to remove them.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfe8552f-08ff-4418-8574-a93d8f37325f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coming into our data science program you will need to know some math and stats however many of our applicants actually learn in the application process  you dont need to be an expert before applying data science is a very accessible field to anyone dedicated to learning new skills and we can work with any applicant to help them learn what they need to know but what skills do we mean exactly just what exactly are the data science math and stats principles you need to know' 'what are the main math principles you need to know to get into codeups data science program'\n"
     ]
    }
   ],
   "source": [
    "# remove anything that is not a through z, a number, a single quote, or whitespace\n",
    "article = re.sub(r\"[^a-z0-9'\\s]\", '', article)\n",
    "print(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4609b60a-dc45-4525-afce-e079472e88c2",
   "metadata": {},
   "source": [
    "#### Tokenization  \n",
    "\n",
    "- After removing non-ASCII characters and special characters, it's common to **tokenize** the strings, to break words and any punctuation left over into discrete units. ***Tokenization is the process of breaking something down into discrete units.*** In the context of NLP, this means breaking text down into discrete words, punctuation, etc.\n",
    "\n",
    "* We will use **`nltk`** to do tokenization for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f1b9c0c-65a5-4901-8b3e-c6b0060c4bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coming into our data science program you will need to know some math and stats however many of our applicants actually learn in the application process you dont need to be an expert before applying data science is a very accessible field to anyone dedicated to learning new skills and we can work with any applicant to help them learn what they need to know but what skills do we mean exactly just what exactly are the data science math and stats principles you need to know ' ' what are the main mat\n"
     ]
    }
   ],
   "source": [
    "tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "\n",
    "print(tokenizer.tokenize(article, return_str=True)[0:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e316066-afdd-4c05-b90e-4e57d5f653af",
   "metadata": {},
   "source": [
    "#### Stemming and Lemmatization  \n",
    "\n",
    "- Usually you will want to use lemmatization. We will demonstrate why that is the case by looking at both here.  \n",
    "\n",
    "- **Stemming** - Word stems are the base form of a word.\n",
    "      \n",
    "\n",
    "- **We create new words by attaching affixes in a process known as inflection. For example, \"calls\", \"called\", and \"calling\" all share the base stem \"call\".**  \n",
    "\n",
    "- The Porter stemmer is based on the algorithm developed by its inventor, Dr. Martin Porter. Originally, the algorithm is said to have had a total of five different phases for reduction of inflections to their stems, where each phase has its own set of rules.  \n",
    "\n",
    "***Note that usually stemming has a fixed set of rules, hence, the root stems may not be lexicographically correct. This means that the stemmed words may not be semantically correct, and might have a chance of not being present in the dictionary (as we'll see in the output of stemming).***  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39c13a85-79d9-45be-b531-c3b2327612af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('call', 'call', 'call')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the nltk stemmer object, then use it\n",
    "ps = nltk.porter.PorterStemmer()\n",
    "\n",
    "ps.stem('call'), ps.stem('called'), ps.stem('calling')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197c044c-32bc-4670-95fd-ff330b0e2c4b",
   "metadata": {},
   "source": [
    "- **Now we can apply this stemming transformation to all the words in the article.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0f84dff-201c-4d33-a921-85c997282a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "come into our data scienc program you will need to know some math and stat howev mani of our applic actual learn in the applic process you dont need to be an expert befor appli data scienc is a veri access field to anyon dedic to learn new skill and we can work with ani applic to help them learn what they need to know but what skill do we mean exactli just what exactli are the data scienc math and stat principl you need to know' 'what are the main math principl you need to know to get into codeup data scienc program'\n"
     ]
    }
   ],
   "source": [
    "stems = [ps.stem(word) for word in article.split()]\n",
    "article_stemmed = ' '.join(stems)\n",
    "print(article_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "543c94ef-0d4d-424e-bec9-545c69c9806b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "to        9\n",
       "need      5\n",
       "data      4\n",
       "scienc    4\n",
       "you       4\n",
       "know      3\n",
       "learn     3\n",
       "and       3\n",
       "math      3\n",
       "the       3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(stems).value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da774131-6ece-4c00-a529-7a16e3e55b0f",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "\n",
    "- Lemmatization is very similar to stemming, however, the base form in this case is known as the root word, but not the root stem. The difference is that the root word is always a lexicographically correct word (present in the dictionary), but the root stem may not be so. Thus, root word, also known as the lemma, will always be present in the dictionary.  \n",
    "\n",
    "- Note that the lemmatization process is considerably slower than stemming, because an additional step is involved where the root form or lemma is formed by removing the affix from the word if and only if the lemma is present in the dictionary.  \n",
    "  \n",
    "- Let's take a look at a simple example of the difference between stemming and lemmatization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11ff2045-25e3-4323-815d-238656599308",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/marc_aradillas/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15fb2a6c-36b5-4e90-98ea-684aec97a5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stem: he -- lemma: He\n",
      "stem: wa -- lemma: wa\n",
      "stem: run -- lemma: running\n",
      "stem: and -- lemma: and\n",
      "stem: eat -- lemma: eating\n",
      "stem: at -- lemma: at\n",
      "stem: same -- lemma: same\n",
      "stem: time. -- lemma: time.\n",
      "stem: he -- lemma: He\n",
      "stem: ha -- lemma: ha\n",
      "stem: bad -- lemma: bad\n",
      "stem: habit -- lemma: habit\n",
      "stem: of -- lemma: of\n",
      "stem: swim -- lemma: swimming\n",
      "stem: after -- lemma: after\n",
      "stem: play -- lemma: playing\n",
      "stem: long -- lemma: long\n",
      "stem: hour -- lemma: hour\n",
      "stem: in -- lemma: in\n",
      "stem: the -- lemma: the\n",
      "stem: sun. -- lemma: Sun.\n"
     ]
    }
   ],
   "source": [
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "\n",
    "for word in sentence.split():\n",
    "    print('stem:', ps.stem(word), '-- lemma:', wnl.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6327b164-8a1e-45b5-9e23-8d4b0fac6d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stem: studi -- lemma: studying\n",
      "stem: what -- lemma: what\n",
      "stem: they -- lemma: they\n",
      "stem: need -- lemma: needed\n",
      "stem: to -- lemma: to\n",
      "stem: study, -- lemma: study,\n",
      "stem: the -- lemma: the\n",
      "stem: student -- lemma: student\n",
      "stem: studi -- lemma: studied\n",
      "stem: studious -- lemma: studiously\n"
     ]
    }
   ],
   "source": [
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "for word in 'studying what they needed to study, the students studied studiously'.split():\n",
    "    print('stem:', ps.stem(word), '-- lemma:', wnl.lemmatize(word))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89a0397-2197-47a9-adc1-cfae9bd337b9",
   "metadata": {},
   "source": [
    "- And now we can apply lemmatization to our entire document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12fd8b85-a243-4f15-92e8-508376c0e928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coming into our data science program you will need to know some math and stats however many of our applicant actually learn in the application process you dont need to be an expert before applying data science is a very accessible field to anyone dedicated to learning new skill and we can work with any applicant to help them learn what they need to know but what skill do we mean exactly just what exactly are the data science math and stats principle you need to know' 'what are the main math principle you need to know to get into codeups data science program'\n"
     ]
    }
   ],
   "source": [
    "lemmas = [wnl.lemmatize(word) for word in article.split()]\n",
    "article_lemmatized = ' '.join(lemmas)\n",
    "\n",
    "print(article_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52902f0c-af32-48f6-bb25-b754decc0554",
   "metadata": {},
   "source": [
    "- Now that we have a list of the lemmas, we can take a look at the most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3438f0c1-3fae-41bf-9f9f-2b3c5315dae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "to         9\n",
       "need       5\n",
       "data       4\n",
       "science    4\n",
       "you        4\n",
       "math       3\n",
       "the        3\n",
       "what       3\n",
       "and        3\n",
       "know       3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(lemmas).value_counts()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31820dfb-74b3-4cb2-8448-0ebd90d30e3e",
   "metadata": {},
   "source": [
    "#### Removing Stopwords\n",
    "\n",
    "- Words which have little or no significance, especially when constructing meaningful features from text, are known as **stop words** (or **stopwords**). These are usually words that end up having the maximum frequency if you do a simple term or word frequency in a corpus. Typically, these can be articles, conjunctions, prepositions and so on. Some examples of stopwords: a, an, the, and like.\n",
    "\n",
    "- While there is no universal stopword list, we will use a standard English language stopwords list from nltk. You can also add your own domain-specific stopwords as needed.\n",
    "\n",
    "- Before removing stopwords, we want to segment text into linguistic units such as words or numbers. This process is called tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ded948e-0b86-4576-a808-1772607a5807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopword_list = stopwords.words('english')\n",
    "\n",
    "stopword_list.remove('no')\n",
    "stopword_list.remove('not')\n",
    "\n",
    "stopword_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716e172e-6bde-44d8-a63e-f05b5f2e78c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
